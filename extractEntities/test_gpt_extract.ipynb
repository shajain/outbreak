{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.organization = os.getenv(\"OPENAI_ORG_ID\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Allows for viewing large data. Can be a boon when data is too large\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import promed dataset and convert it to string\n",
    "promed = pd.read_json(\"C:\\\\Users\\\\Rushali\\\\Documents\\\\Code\\\\Lab\\\\CHAIN\\\\Data\\\\ProMED\\\\promed_1.json\")\n",
    "promed['header'] = promed['header'].astype(str)\n",
    "# promed['header'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8DZhDWmtuiBRi4X4WHuh93dlwFOd2 at 0x19267f7deb0> JSON: {\n",
       "  \"id\": \"chatcmpl-8DZhDWmtuiBRi4X4WHuh93dlwFOd2\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1698246171,\n",
       "  \"model\": \"gpt-3.5-turbo-0613\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"content\": \"disease outbreak name;country name\\nAvian influenza;Kuwait, Myanmar\\nTularemia;Taiwan\\nAmerican foulbrood, apis;Fiji\\nCOVID-19;USA\\nLumpy skin disease;Bulgaria\\nAvian cholera;USA\\nNorovirus;Spain\\nFoot & mouth disease, bovine;South Korea\\nCharcoal rot, soybean;USA\\nDengue;USA\"\n",
       "      },\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 650,\n",
       "    \"completion_tokens\": 89,\n",
       "    \"total_tokens\": 739\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Input command is quite long. GPT3.5 tends to not follow all instructions as the number of intructions keep increasing. \n",
    "# Testing on first 10 rows.\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format. The columns are disease outbreak name and country name. The values for each column must be separated by a semicolon. The values for each row must begin on a new line. The value for country must be only the name of the country and no other characters are permitted. The value for disease must be only the name of the disease and no other characters are permitted\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'].head(10).to_string()}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['disease outbreak name', 'country name'], ['Avian influenza', 'Kuwait, Myanmar'], ['Tularemia', 'Taiwan'], ['American foulbrood, apis', 'Fiji'], ['COVID-19', 'USA'], ['Lumpy skin disease', 'Bulgaria'], ['Avian cholera', 'USA'], ['Norovirus', 'Spain'], ['Foot & mouth disease, bovine', 'South Korea'], ['Charcoal rot, soybean', 'USA'], ['Dengue', 'USA']]\n",
      "['Avian influenza', 'Kuwait, Myanmar']\n",
      "Kuwait, Myanmar\n"
     ]
    }
   ],
   "source": [
    "# Extracting the output from GPT response and formatting it for ease of use.\n",
    "\n",
    "# response.choices[0].message.content.split('\\n')\n",
    "# response.choices[0].message.content.split('\\n')[1]\n",
    "\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "pro = list(map(lambda x: x.split(';'), r))\n",
    "\n",
    "print(pro)\n",
    "print(pro[1])\n",
    "print(pro[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['disease outbreak name', 'country name'], ['Avian influenza', 'USA'], ['Ebola', 'Congo DR'], ['Jaundice', 'Bangladesh'], ['Cholera', 'South Sudan'], ['Ricin', 'USA'], ['Rift Valley fever', 'Mauritania'], ['Herpes B virus infection', 'USA'], ['West Nile virus', 'USA'], ['E. coli EHEC', 'Australia'], ['BSE', 'USA']]\n",
      "['Avian influenza', 'USA']\n",
      "USA\n"
     ]
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Testing on last 10 rows.\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format. The columns are disease outbreak name and country name. The values for each column must be separated by a semicolon. The values for each row must begin on a new line. The value for country must be only the name of the country and no other characters are permitted. The value for disease must be only the name of the disease and no other characters are permitted\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'].tail(10).to_string()}\n",
    "    ]\n",
    ")\n",
    "\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "pro = list(map(lambda x: x.split(';'), r))\n",
    "\n",
    "print(pro)\n",
    "print(pro[1])\n",
    "print(pro[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['disease outbreak name', 'country name'], ['Late blight, potato', 'Papua New Guinea'], ['Vesicular stomatitis', 'USA'], ['E. coli EHEC', 'USA'], ['E. coli O157', 'USA'], ['Cholera, diarrhea & dysentery update', 'USA'], ['Hantavirus update 2011 - Americas', 'Chile'], ['Drugs, fish, contaminated waterways', 'USA'], ['Anthrax, human, equine', 'Kyrgyzstan'], ['Cryptosporidiosis', 'Sweden'], ['Meningitis advisory', 'Burundi & Congo, Dem.Rep.']]\n",
      "['Late blight, potato', 'Papua New Guinea']\n",
      "Papua New Guinea\n"
     ]
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Testing on rows 50-60.\n",
    "# Repeatedly running this instruction can sometimes lead to different results.\n",
    "# Especially if you run it on a single row at a time.\n",
    "\n",
    "# Results are not as expected. Countries are extracted properly and disease names aren't extracted very well.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to parse it into CSV format. The columns are disease outbreak name and country name. The values for each column must be separated by a semicolon. The values for each row must begin on a new line. The value for country must be only the name of the country and no other characters are permitted. The value for disease must be only contain the name of the disease and no other characters are permitted\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][50:60].to_string()}\n",
    "    ]\n",
    ")\n",
    "\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "pro = list(map(lambda x: x.split(';'), r))\n",
    "\n",
    "print(pro)\n",
    "print(pro[1])\n",
    "print(pro[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['disease outbreak name', 'country name'],\n",
       " ['Late blight, potato', 'Papua New Guinea'],\n",
       " ['Vesicular stomatitis', 'USA'],\n",
       " ['E. coli EHEC', 'USA'],\n",
       " ['E. coli O157', 'USA'],\n",
       " ['Cholera, diarrhea & dysentery update', 'USA'],\n",
       " ['Hantavirus update 2011 - Americas', 'Chile'],\n",
       " ['Drugs, fish, contaminated waterways', 'USA'],\n",
       " ['Anthrax, human, equine', 'Kyrgyzstan'],\n",
       " ['Cryptosporidiosis', 'Sweden'],\n",
       " ['Meningitis advisory', 'Burundi & Congo, Dem.Rep.']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Tested on rows 50-60.\n",
    "# Repeatedly running the previous cell led to erreneous results.\n",
    "# Extracted country where there was nothing to extract. Fifth entry \n",
    "\n",
    "pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disease;country\\nLate blight, potato;Papua New Guinea\\nVesicular stomatitis;USA\\nE. coli EHEC;USA\\nE. coli O157;USA\\nCholera, diarrhea & dysentery update;unknown\\nHantavirus update;unknown\\nDrugs, fish, contaminated waterways;USA\\nAnthrax, human, equine;Kyrgyzstan\\nCryptosporidiosis;Sweden\\nMeningitis advisory;Burundi & Congo, Dem.Rep.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Testing on rows 50-60.\n",
    "# Repeatedly running this instruction can sometimes lead to different results.\n",
    "# Input command was modified to ask to rewrite country name\n",
    "\n",
    "# Results are not as expected. Countries are rewritten properly and disease names aren't extracted very well.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format. The disease name should be only the medical name of all the diseases mentioned and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data. The values for each column must be separated by a semicolon. The values for each row must begin on a new line.\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][50:60].to_string()}\n",
    "    ]\n",
    ")\n",
    "r=response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disease;country',\n",
       " 'Late blight, potato;Papua New Guinea',\n",
       " 'Vesicular stomatitis;USA',\n",
       " 'E. coli EHEC;USA',\n",
       " 'E. coli O157;USA',\n",
       " 'Cholera, diarrhea & dysentery update;unknown',\n",
       " 'Hantavirus update;unknown',\n",
       " 'Drugs, fish, contaminated waterways;USA',\n",
       " 'Anthrax, human, equine;Kyrgyzstan',\n",
       " 'Cryptosporidiosis;Sweden',\n",
       " 'Meningitis advisory;Burundi & Congo, Dem.Rep.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Tested on rows 50-60.\n",
    "# Repeatedly running the previous cell led to erreneous results.\n",
    "# Extracted nothing where there was a country to extract. Sixth entry \n",
    "r.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Disease: Late blight, potato',\n",
       " 'Country: Papua New Guinea',\n",
       " '',\n",
       " 'Disease: Vesicular stomatitis',\n",
       " 'Country: USA',\n",
       " '',\n",
       " 'Disease: E. coli EHEC',\n",
       " 'Country: USA',\n",
       " '',\n",
       " 'Disease: E. coli O157',\n",
       " 'Country: USA',\n",
       " '',\n",
       " 'Disease: Cholera, diarrhea & dysentery',\n",
       " 'Country: Not mentioned',\n",
       " '',\n",
       " 'Disease: Hantavirus',\n",
       " 'Country: Americas',\n",
       " '',\n",
       " 'Disease: Drugs, fish, contaminated waterways',\n",
       " 'Country: USA',\n",
       " '',\n",
       " 'Disease: Anthrax, human, equine',\n",
       " 'Country: Kyrgyzstan',\n",
       " '',\n",
       " 'Disease: Cryptosporidiosis',\n",
       " 'Country: Sweden',\n",
       " '',\n",
       " 'Disease: Meningitis advisory',\n",
       " 'Country: Burundi & Congo, Dem.Rep.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT3.5\n",
    "# Testing on rows 50-60.\n",
    "# Input command was modified to ask to rewrite country name and not format as CSV\n",
    "\n",
    "# Results are not as expected. Countries are rewritten or extracted properly and disease names aren't extracted very well.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country. The disease name should be only the medical name of all the diseases and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data.\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][50:60].to_string()}\n",
    "    ]\n",
    ")\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Published Date: 2007-05-11 11:00:02 EDT\\\\nSubject: PRO/EDR> Cholera, diarrhea & dysentery update 2007 (20)\\\\nArchive Number: 20070511.1509']\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promed['header'][54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Disease Name: Meningitis',\n",
       " 'Country Name: Burundi, Democratic Republic of Congo']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT4\n",
    "# Testing on rows 59\n",
    "# Works really well\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country. The disease name should contain only the medical name of all the diseases and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data.\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][59]}\n",
    "    ]\n",
    ")\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Disease: Late blight, potato; Country: Papua New Guinea',\n",
       " '2. Disease: Vesicular stomatitis; Country: United States of America',\n",
       " '3. Disease: E. coli EHEC; Country: United States of America',\n",
       " '4. Disease: E. coli O157; Country: United States of America',\n",
       " '5. Disease: Cholera, diarrhea & dysentery; Country: No specific country mentioned',\n",
       " '6. Disease: Hantavirus; Countries: Chile, United States of America',\n",
       " \"7. Not a disease, it's about Drugs, fish, contaminated waterways; Country: United States of America\",\n",
       " '8. Disease: Anthrax, human, equine; Country: Kyrgyzstan',\n",
       " '9. Disease: Cryptosporidiosis; Country: Sweden',\n",
       " '10. Disease: Meningitis; Countries: Burundi, Democratic Republic of Congo']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT4\n",
    "# Testing on rows 50-60\n",
    "# Continues to work really well\n",
    "# Input command doesn't ask to parse into CSV format\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country. The disease name should contain only the medical name of all the diseases and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data.\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][50:60].to_string()}\n",
    "    ]\n",
    ")\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Late blight, potato\",\"Papua New Guinea\"',\n",
       " '\"Vesicular stomatitis\",\"United States of America\"',\n",
       " '\"E. coli EHEC\",\"United States of America\"',\n",
       " '\"E. coli O157\",\"United States of America\"',\n",
       " '\"Cholera, diarrhea & dysentery\",\"Not specified\"',\n",
       " '\"Hantavirus\",\"Chile, United States of America\"',\n",
       " '\"Drugs, fish, contaminated waterways\",\"United States of America\"',\n",
       " '\"Anthrax, human, equine\",\"Kyrgyzstan\"',\n",
       " '\"Cryptosporidiosis\",\"Sweden\"',\n",
       " '\"Meningitis advisory\",\"Burundi, Democratic Republic of the Congo\"']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GPT4\n",
    "# Testing on rows 50-60\n",
    "# Continues to work really well\n",
    "# Input command asks to parse into CSV format to reduce number of token used.\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format. The disease name should contain only the medical name of all the diseases and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data.\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][50:60].to_string()}\n",
    "    ]\n",
    ")\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Published Date: 2009-04-01 18:00:52 EDT\\\\nSubject: PRO/AH/EDR> Drugs, fish, contaminated waterways - USA (02)\\\\nArchive Number: 20090401.1256']\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promed['header'][56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Late blight, potato\";\"Papua New Guinea\"',\n",
       " '\"Vesicular stomatitis\";\"United States of America\"',\n",
       " '\"E. coli EHEC\";\"United States of America\"',\n",
       " '\"E. coli O157\";\"United States of America\"',\n",
       " '\"Cholera, diarrhea & dysentery\";\"Not specified\"',\n",
       " '\"Hantavirus\";\"Chile, United States of America\"',\n",
       " '\"Drugs, fish, contaminated waterways\";\"United States of America\"',\n",
       " '\"Anthrax, human, equine\";\"Kyrgyzstan\"',\n",
       " '\"Cryptosporidiosis\";\"Sweden\"',\n",
       " '\"Meningitis\";\"Burundi, Democratic Republic of the Congo\"']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing extraction\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format where the columns are seperated by a semicolon. The disease name should contain only the medical name of all the diseases and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data.\"},\n",
    "        {\"role\": \"user\", \"content\": promed['header'][50:60].to_string()}\n",
    "    ],\n",
    "    seed = 1\n",
    ")\n",
    "r=response.choices[0].message.content.split('\\n')\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  3\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# Disregard this cell. This code worked only before Dev Day. After that, this prompt generated extremely poor results.\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.organization = os.getenv(\"OPENAI_ORG_ID\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#Allows for viewing large data. Can be a boon when data is too large\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ") \n",
    "import backoff\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    global end_index\n",
    "    print(\"Too many tokens\")\n",
    "    end_index = end_index-10\n",
    "\n",
    "def backoff_hdlr2(details):\n",
    "    global flag\n",
    "    print(\"RateLimitError, too many requests\")\n",
    "    flag = True\n",
    "\n",
    "def backoff_hdlr3(details):\n",
    "    global flag\n",
    "    flag = True\n",
    "    print(\"APIError or Timeout, either way something is wrong with openai \\n\", details)\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.Timeout, on_backoff=backoff_hdlr3, max_tries = 2, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.APIError,on_backoff=backoff_hdlr3, max_tries = 2, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError,on_backoff=backoff_hdlr2, max_tries = 3, max_time=120)\n",
    "@backoff.on_exception(backoff.expo, openai.error.InvalidRequestError,  on_backoff=backoff_hdlr, max_tries = 2)\n",
    "def completion_with_backoff(promed):\n",
    "        global end_index, start_index, flag\n",
    "        print(start_index)\n",
    "    # try: \n",
    "        response = openai.ChatCompletion.create(\n",
    "            # model=\"gpt-4\",\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            # model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format where the columns are seperated by a semicolon. The disease name should contain only the medical name of all the diseases and return no extra data. All the counries mentioned must be extracted and seperated by commas.The country name should be rewritten with the full name of the country with no acronyms and no extra data. Return None when there is no country or disease mentioned. Do not return any text excepted the extracted values. \"},\n",
    "                {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, extract only the disease name and country name in a CSV format where the columns are seperated by a semicolon. All the countries mentioned must be extracted and seperated by commas. All diseases mentioned must be extracted and seperated by commas. The disease name should contain only the medical name of all the diseases and return no extra data. The country name should be rewritten with the full name of the country with no acronyms and no extra data. Return 'Nil' when there is no country or disease to be extracted. Do not return any text/characters excepted the extracted values.\"},\n",
    "                {\"role\": \"user\", \"content\": promed['header'][start_index:end_index].to_string()}\n",
    "            ],\n",
    "            seed = 1\n",
    "        )\n",
    "    # except openai.error.Timeout as e:\n",
    "    #     # Handle timeout error, e.g. retry or log\n",
    "    #     print(f\"OpenAI API request timed out: {e}\")\n",
    "    #     pass\n",
    "    # except openai.error.APIError as e:\n",
    "    #     # Handle API error, e.g. retry or log\n",
    "    #     print(f\"OpenAI API returned an API Error: {e}\")\n",
    "    #     pass\n",
    "    # except openai.error.APIConnectionError as e:\n",
    "    #     # Handle connection error, e.g. check network or log\n",
    "    #     print(f\"OpenAI API request failed to connect: {e}\")\n",
    "    #     pass\n",
    "    # except openai.error.InvalidRequestError as e:\n",
    "    #     # Handle invalid request error, e.g. validate parameters or log\n",
    "    #     print(f\"OpenAI API request was invalid: {e}\")\n",
    "    #     pass\n",
    "    # except openai.error.AuthenticationError as e:\n",
    "    #     # Handle authentication error, e.g. check credentials or log\n",
    "    #     print(f\"OpenAI API request was not authorized: {e}\")\n",
    "    #     pass\n",
    "    # except openai.error.PermissionError as e:\n",
    "    #     # Handle permission error, e.g. check scope or log\n",
    "    #     print(f\"OpenAI API request was not permitted: {e}\")\n",
    "    #     pass\n",
    "    # except openai.error.RateLimitError as e:\n",
    "    #     # Handle rate limit error, e.g. wait or log\n",
    "    #     print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "    #     pass\n",
    "    # except Exception as e:\n",
    "    #     # Handle rate limit error, e.g. wait or log\n",
    "    #     print(e)\n",
    "    #     pass\n",
    "\n",
    "        flag = False\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# promed_test['header'].count()\n",
    "# for i in range(1,14,1):\n",
    "i=3\n",
    "if i==3:\n",
    "    #Import promed dataset and convert it to string\n",
    "    print(\"File \", i)\n",
    "    filename = \"promed_\"+str(i)+\".json\"\n",
    "    promed_test = pd.read_json(\"C:\\\\Users\\\\Rushali\\\\Documents\\\\Code\\\\Lab\\\\CHAIN\\\\Data\\\\ProMED\\\\\"+filename)\n",
    "    promed_test['header'] = promed_test['header'].astype(str)\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 50\n",
    "    flag = False\n",
    "\n",
    "    file = open(\"extract\"+str(i)+\".csv\", 'ab')\n",
    "\n",
    "    while start_index < promed_test['header'].count():\n",
    "        try:\n",
    "            if flag:\n",
    "                break\n",
    "            if end_index > promed_test['header'].count():\n",
    "                end_index = promed_test['header'].count()\n",
    "\n",
    "            resp = completion_with_backoff(promed_test)\n",
    "            np.savetxt(file, resp.choices[0].message.content.split('\\n')[:], delimiter=\";\", fmt ='% s')\n",
    "            start_index = end_index\n",
    "            end_index = end_index + 50\n",
    "        except Exception as e:\n",
    "            print(\"IN LOOP\")\n",
    "            print(e)\n",
    "            break\n",
    "        # print(end_index)\n",
    "    print(\"END\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File  1\n",
      "4950\n",
      "50    50  lines and ;\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# Main code to extract country and disease from promed alerts using gpt\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import openai\n",
    "openai.organization = os.getenv(\"OPENAI_ORG_ID\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import pandas as pd\n",
    "#Allows for viewing large data. Can be a boon when data is too large\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ") \n",
    "import backoff\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    global end_index\n",
    "    print(\"Too many tokens\")\n",
    "    end_index = end_index-10\n",
    "\n",
    "def backoff_hdlr2(details):\n",
    "    global flag\n",
    "    print(\"RateLimitError, too many requests\")\n",
    "    flag = True\n",
    "\n",
    "def backoff_hdlr3(details):\n",
    "    global flag\n",
    "    flag = True\n",
    "    print(\"APIError or Timeout, either way something is wrong with openai \\n\", details)\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.Timeout, on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.APIError,on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError,on_backoff=backoff_hdlr2, max_tries = 5, max_time=120)\n",
    "@backoff.on_exception(backoff.expo, openai.error.InvalidRequestError,  on_backoff=backoff_hdlr, max_tries = 5)\n",
    "def completion_with_backoff(promed):\n",
    "        global end_index, start_index, flag\n",
    "        print(start_index)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            # model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format where the columns are seperated by a semicolon. The disease name should contain only the medical name of all the diseases and return no extra data. All the counries mentioned must be extracted and seperated by commas.The country name should be rewritten with the full name of the country with no acronyms and no extra data. Return None when there is no country or disease mentioned. Do not return any text excepted the extracted values. \"},\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format where the columns are seperated by a semicolon. \"},\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with 50 data points. Your task is to extract the diseases and countries mentioned in each data point. The output you provide should contain exactly 50 lines where every disease or country mentioned is extracted and is comma seperated. The diseases and countries are seperated by a semicolon. The disease name should be rewtitten with the complete medical name of that disease. The country name should be rewritten with the full name of the country with no acronyms. Return 'Nil' when there is no country or disease to be extracted. Do not return any text/characters that do not corespond to the name of a country or a disease.\"},\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with 50 data points. Your task is to extract the diseases and countries mentioned in each data point. The output you provide should contain exactly 50 lines where each line has at least one entry for disease and at least one for country. Every disease or country mentioned must be extracted and comma seperated. The diseases and countries are seperated by a semicolon. The disease name should be rewtitten with the complete medical name of that disease. The country name should be rewritten with the full name of the country with no acronyms. Return 'Nil' when there is no country or disease to be extracted. Do not return any extra data. 'Published Date: 2007-03-30 10:00:01 EDT\\\\nSubject: PRO/EDR> Poliomyelitis - Worldwide (02): Nigeria, DR Congo, Somalia, Pakistan\\\\nArchive Number: 20070330.1090' is a sample datapoint. The output for this should be 'Poliomyelitis;Nigeria, Democratic Republic of the Congo, Somalia, Pakistan'. \" },\n",
    "                {\"role\": \"system\", \"content\": \"You will be provided with 50 data points. Your task is to extract the diseases and countries mentioned in each data point. The output you provide should contain exactly 50 lines where each line has at least one entry for disease and at least one for country. Every disease or country mentioned must be extracted and comma seperated. The diseases and countries are seperated by a semicolon. Return 'Nil' when there is no country or disease to be extracted. Do not return any extra data. '['Published Date: 2007-03-30 10:00:01 EDT\\\\nSubject: PRO/EDR> Poliomyelitis - Worldwide (02): Nigeria, DR Congo, Somalia, Pakistan\\\\nArchive Number: 20070330.1090' , 'Published Date: 2005-06-24 19:50:00 EDT\\\\nSubject: PRO/EDR> Cholera, diarrhea & dysentery update 2005 (24)\\\\nArchive Number: 20050624.1775' , 'Published Date: 2000-01-31 18:50:00 EST\\\\nSubject: PRO> updates about situation- Bangladesh (02)\\\\nArchive Number: 20000131.0147' , 'Published Date: 2010-05-06 14:00:03 EDT\\\\nSubject: PRO/AH/EDR> about about vaccines \\\\nArchive Number: 20100506.1477'] is a sample with 4 datapoints. The output for this should be 'Poliomyelitis;Nigeria, Democratic Republic of the Congo, Somalia, Pakistan \\n Cholera, diarrhea & dysentery;Nil \\n Nil; Bangladesh \\n Nil;Nil'\" },\n",
    "                {\"role\": \"user\", \"content\": promed['header'][start_index:end_index].to_string()}\n",
    "            ],\n",
    "            seed = 1\n",
    "        )\n",
    "        # print(promed['header'][start_index:end_index].to_string())\n",
    "        flag = False\n",
    "        return response\n",
    "\n",
    "\n",
    "# Redo was necessary as some blocks of alerts would continuously throw errors with the previous seed.\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.Timeout, on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.APIError,on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError,on_backoff=backoff_hdlr2, max_tries = 5, max_time=120)\n",
    "@backoff.on_exception(backoff.expo, openai.error.InvalidRequestError,  on_backoff=backoff_hdlr, max_tries = 5)\n",
    "def completion_changed_seed(promed):\n",
    "        global end_index, start_index, flag\n",
    "        print(\"Redo \",start_index)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            # model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with unstructured data, and your task is to extract only the disease name and the name of the country in a CSV format where the columns are seperated by a semicolon. The disease name should contain only the medical name of all the diseases and return no extra data. All the counries mentioned must be extracted and seperated by commas.The country name should be rewritten with the full name of the country with no acronyms and no extra data. Return None when there is no country or disease mentioned. Do not return any text excepted the extracted values. \"},\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with 50 data points. Your task is to extract the diseases and countries mentioned in each data point. The output you provide should contain exactly 50 lines where each line has at least one entry for disease and at least one for country. Every disease or country mentioned must be extracted and comma seperated. The diseases and countries are seperated by a semicolon. The disease name should be rewtitten with the complete medical name of that disease. The country name should be rewritten with the full name of the country with no acronyms. Return 'Nil' when there is no country or disease to be extracted. Do not return any extra data.\"},\n",
    "                # {\"role\": \"system\", \"content\": \"You will be provided with 50 data points. Your task is to extract the diseases and countries mentioned in each data point. The output you provide should contain exactly 50 lines where each line has at least one entry for disease and at least one for country. Every disease or country mentioned must be extracted and comma seperated. The diseases and countries are seperated by a semicolon. The disease name should be rewtitten with the complete medical name of that disease. The country name should be rewritten with the full name of the country with no acronyms. Return 'Nil' when there is no country or disease to be extracted. Do not return any extra data. 'Published Date: 2007-03-30 10:00:01 EDT\\\\nSubject: PRO/EDR> Poliomyelitis - Worldwide (02): Nigeria, DR Congo, Somalia, Pakistan\\\\nArchive Number: 20070330.1090' is a sample datapoint. The output for this should be 'Poliomyelitis;Nigeria, Democratic Republic of the Congo, Somalia, Pakistan'. \" },\n",
    "                {\"role\": \"system\", \"content\": \"You will be provided with 50 data points. Your task is to extract the diseases and countries mentioned in each data point. The output you provide should contain exactly 50 lines where each line has at least one entry for disease and at least one for country. Every disease or country mentioned must be extracted and comma seperated. The diseases and countries are seperated by a semicolon. The disease name should be rewtitten with the complete medical name of that disease. The country name should be rewritten with the full name of the country with no acronyms. Return 'Nil' when there is no country or disease to be extracted. Do not return any extra data. '['Published Date: 2007-03-30 10:00:01 EDT\\\\nSubject: PRO/EDR> Poliomyelitis - Worldwide (02): Nigeria, DR Congo, Somalia, Pakistan\\\\nArchive Number: 20070330.1090' , 'Published Date: 2005-06-24 19:50:00 EDT\\\\nSubject: PRO/EDR> Cholera, diarrhea & dysentery update 2005 (24)\\\\nArchive Number: 20050624.1775' , 'Published Date: 2000-01-31 18:50:00 EST\\\\nSubject: PRO> updates about situation- Bangladesh (02)\\\\nArchive Number: 20000131.0147' , 'Published Date: 2010-05-06 14:00:03 EDT\\\\nSubject: PRO/AH/EDR> about about vaccines \\\\nArchive Number: 20100506.1477'] is a sample datapoint. The output for this should be 'Poliomyelitis;Nigeria, Democratic Republic of the Congo, Somalia, Pakistan \\n Cholera, diarrhea & dysentery;Nil \\n Nil; Bangladesh \\n Nil;Nil'\" },\n",
    "                {\"role\": \"user\", \"content\": promed['header'][start_index:end_index].to_string()}\n",
    "            ],\n",
    "            seed = 1\n",
    "        )\n",
    "        flag = False\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "# Ideally perform only one or two files in series as chatgpt tends to randomly throw API errors which can not be resolved.\n",
    "\n",
    "for i in range(1,14,1):\n",
    "\n",
    "# i=1\n",
    "# if i==1:\n",
    "    \n",
    "    #Import promed dataset and convert it to string\n",
    "    print(\"File \", i)\n",
    "    filename = \"promed_\"+str(i)+\".json\"\n",
    "    promed_test = pd.read_json(\"C:\\\\Users\\\\Rushali\\\\Documents\\\\Code\\\\Lab\\\\CHAIN\\\\Data\\\\ProMED\\\\\"+filename)\n",
    "    promed_test['header'] = promed_test['header'].astype(str)\n",
    "\n",
    "    start_index = 0\n",
    "    end_index = 50\n",
    "    flag = False\n",
    "\n",
    "    while start_index < promed_test['header'].count():\n",
    "\n",
    "        try:\n",
    "\n",
    "            if end_index > promed_test['header'].count():\n",
    "                end_index = promed_test['header'].count()\n",
    "\n",
    "            resp = completion_with_backoff(promed_test)\n",
    "\n",
    "            if len(resp.choices[0].message.content.split('\\n')[:]) != 50:\n",
    "                resp = completion_changed_seed(promed_test)\n",
    "\n",
    "            if resp.choices[0].message.content.count(\";\") != 50:\n",
    "                resp = completion_changed_seed(promed_test)\n",
    "\n",
    "            print(len(resp.choices[0].message.content.split('\\n')[:]) , \"  \", resp.choices[0].message.content.count(\";\"), \" lines and ;\" )\n",
    "\n",
    "            filename = str(i) + '_' + str(start_index)\n",
    "            out_file = open(\"bin_responses\\\\\"+filename+\".json\", \"w\") \n",
    "            json.dump(resp, out_file,ensure_ascii=False) \n",
    "            out_file.close()\n",
    "\n",
    "            file = open(\"extraction\\\\extract\"+str(i)+\".csv\", 'a')\n",
    "            np.savetxt(file, resp.choices[0].message.content.split('\\n')[:], delimiter=\";\", fmt ='% s')\n",
    "            file.close()\n",
    "\n",
    "            start_index = end_index\n",
    "            end_index = end_index + 50\n",
    "        except Exception as e:\n",
    "            print(\"IN LOOP\")\n",
    "            print(e)\n",
    "            break\n",
    "        # print(end_index)\n",
    "    print(\"END\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "promed_csv = pd.read_csv(\"C:\\\\Users\\\\Rushali\\\\Documents\\\\Code\\\\Lab\\\\outbreak\\\\extractEntities\\\\extraction\\\\extract4.csv\", sep=\";\",names=[\"Disease\",\"Country\"])\n",
    "filename = \"promed_\"+str(13)+\".json\"\n",
    "promed_test = pd.read_json(\"C:\\\\Users\\\\Rushali\\\\Documents\\\\Code\\\\Lab\\\\CHAIN\\\\Data\\\\ProMED\\\\\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4730"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prelim count of number of diseases extracted\n",
    "len(pd.unique(promed_csv[\"Disease\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4264\n",
      "Disease    Gastroenteritis, Vibrio, E. coli\n",
      "Country                                 USA\n",
      "Name: 4264, dtype: object\n",
      "['Published Date: 2021-12-22 13:47:18 EST\\nSubject: PRO/EDR> Gastroenteritis - USA (03): cruise ship, Vibrio, E. coli\\nArchive Number: 20211222.8700371']\n",
      "header    4544\n",
      "body      4544\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Random testing to checck if GPT extractions were successful\n",
    "index = np.random.randint(0,5000)\n",
    "print(index)\n",
    "print(promed_csv.iloc[index])\n",
    "print(promed_test['header'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "END\n"
     ]
    }
   ],
   "source": [
    "# Code to extract countries and diseases from WHO data.\n",
    "\n",
    "import os\n",
    "import json \n",
    "\n",
    "import openai\n",
    "openai.organization = os.getenv(\"OPENAI_ORG_ID\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "import pandas as pd\n",
    "#Allows for viewing large data. Can be a boon when data is too large\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ") \n",
    "import backoff\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def backoff_hdlr(details):\n",
    "    global end_index\n",
    "    print(\"Too many tokens\")\n",
    "    end_index = end_index-10\n",
    "\n",
    "def backoff_hdlr2(details):\n",
    "    global flag\n",
    "    print(\"RateLimitError, too many requests\")\n",
    "    flag = True\n",
    "\n",
    "def backoff_hdlr3(details):\n",
    "    global flag\n",
    "    flag = True\n",
    "    print(\"APIError or Timeout, either way something is wrong with openai \\n\", details)\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.Timeout, on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.APIError,on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError,on_backoff=backoff_hdlr2, max_tries = 5, max_time=120)\n",
    "@backoff.on_exception(backoff.expo, openai.error.InvalidRequestError,  on_backoff=backoff_hdlr, max_tries = 5)\n",
    "def completion_with_backoff(who):\n",
    "        global start_index, flag\n",
    "        print(start_index)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            # model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You will be shown snippets of news articles. You need to extract only the country and disease mentioned in JSON. Use Nil if there is nothing to extract.\" },\n",
    "                {\"role\": \"user\", \"content\": who}\n",
    "            ],\n",
    "            seed = 1\n",
    "        )\n",
    "        # print(promed['header'][start_index:end_index].to_string())\n",
    "        flag = False\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.Timeout, on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.APIError,on_backoff=backoff_hdlr3, max_tries = 5, max_time=60)\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError,on_backoff=backoff_hdlr2, max_tries = 5, max_time=120)\n",
    "@backoff.on_exception(backoff.expo, openai.error.InvalidRequestError,  on_backoff=backoff_hdlr, max_tries = 5)\n",
    "def completion_changed_seed(who):\n",
    "        global start_index, flag\n",
    "        print(\"Redo \",start_index)\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            # model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You will be shown snippets of news articles. You need to extract only the country and disease mentioned in JSON. Use Nil if there is nothing to extract.\" },\n",
    "                {\"role\": \"user\", \"content\": who}\n",
    "            ],\n",
    "            seed = 1\n",
    "        )\n",
    "        flag = False\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "# Replace with correct location of WHO file\n",
    "who = pd.read_json(\"C:\\\\Users\\\\Rushali\\\\Documents\\\\Code\\\\Lab\\\\outbreak\\\\data\\\\WHO DON\\\\1695995527.14841_whodonreports.json\")\n",
    "\n",
    "\n",
    "i=1\n",
    "if i==1:\n",
    "    start_index = 0\n",
    "    flag = False\n",
    "\n",
    "    while start_index < 716:\n",
    "        try:\n",
    "            resp = completion_with_backoff(who['Report_text'][start_index][:500])\n",
    "\n",
    "            filename = 'who_' + str(start_index)\n",
    "            out_file = open(\"bin_responses\\\\\"+filename+\".json\", \"w\") \n",
    "            json.dump(resp, out_file,ensure_ascii=False) \n",
    "            out_file.close()\n",
    "\n",
    "            start_index = start_index + 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"IN LOOP\")\n",
    "            print(e)\n",
    "            break\n",
    "        # print(end_index)\n",
    "    print(\"END\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
